backend/docs/business-logic.md
# Performance Monitoring Orchestration Platform - Business Logic Documentation

## Executive Summary

This platform automates the end-to-end process of discovering, monitoring, analyzing, and reporting on website performance. It eliminates manual scripting for performance engineers by leveraging AI to intelligently discover user journeys, execute Lighthouse audits, and generate actionable insights with historical trend analysis.

---

## Core Business Concepts

### 1. Website Entity

A **Website** is a target application to be monitored. It includes:
- **URL**: Primary entry point (must be valid and publicly accessible)
- **Name**: Human-readable identifier (auto-generated by AI if not provided)
- **Description**: Context about the site's purpose (auto-generated by AI if not provided)
- **Status**: Active, Paused, Archived
- **Created/Updated timestamps**

**Business Rules:**
- Duplicate URLs allowed (different monitoring configs)
- Archived websites retain historical data but cannot run new monitoring sessions

---

### 2. User Journey Discovery

AI-powered browser automation automatically discovers realistic user journeys—no manual scripting required.

**Process Flow:**
1. AI agent receives website URL
2. Browser automation (Playwright/Puppeteer via MCP) opens the site
3. AI analyzes page structure and interactive elements
4. AI emulates 4-5 typical user actions (clicks, navigation, forms)
5. Each step's URL/state is captured and stored
6. Optional: Generate reusable automation script

**Business Rules:**
- 5-minute timeout per website
- Max 5 steps per journey
- Steps must be unique URLs or distinct states
- Manual journey definition fallback if discovery fails
- Discovered journeys are versioned

**AI Criteria:**
- Prioritize primary navigation
- Focus on user-facing functionality
- Avoid destructive actions (purchases, deletions)

---

### 3. Performance Monitoring Run

A **Monitoring Run** executes performance tests across all steps in a user journey.

**Execution Flow:**
1. User initiates run for a website
2. System retrieves latest user journey
3. For each step:
   - Launch Lighthouse audit
   - Collect Core Web Vitals and metrics
   - Capture screenshots and diagnostics
4. Store raw results in database
5. Trigger AI analysis pipeline

**Collected Metrics:**
- **Core Web Vitals**: LCP, FID, CLS
- **Additional**: FCP, TTFB, TTI, TBT
- **Scores**: Performance, Accessibility, Best Practices, SEO (0-100)
- **Resource**: Page size, request count, JS execution time
- **Diagnostics**: Unused CSS/JS, render-blocking resources, cache policies

**Business Rules:**
- Only one active run per website at a time
- Failed steps marked as failed, run continues
- 10-minute run timeout
- Minimum 5-minute interval between runs for same website
- Runs are atomic (all-or-nothing for data consistency)

---

### 4. AI-Driven Analysis & Reporting

After metrics collection, AI analyzes results to produce human-readable insights.

**Analysis Components:**
- Performance assessment (overall health, per page)
- Issue identification
- Root cause analysis
- Impact quantification (business impact)
- Recommendations (prioritized, actionable)
- Comparative context (historical, industry benchmarks)

**Report Formats:**
- Markdown (in-app, version control)
- PDF (executive summaries)
- JSON (programmatic access)

**Business Rules:**
- Reports are immutable once generated
- 2-minute report generation timeout
- Failed AI analysis still saves raw metrics
- Reports reference specific run IDs
- All reports stored in object storage (S3-compatible)

---

### 5. Historical Comparison & Trend Analysis

Tracks performance evolution to identify patterns, regressions, and improvements.

**Capabilities:**
- Compare any two runs side-by-side
- Visualize metric trends
- Detect anomalies (spikes/drops)
- Correlate changes with time/events
- Identify degradation patterns

**Business Intelligence:**
- Trend detection (moving averages, regression)
- Seasonality (recurring patterns)
- Event correlation (deployments, traffic)
- SLA tracking
- Forecasting (predict future performance)

**Business Rules:**
- Historical data retained indefinitely unless deleted
- Minimum 3 runs for trend analysis
- Comparisons only valid for same website & journey version
- Anomaly detection requires at least 10 historical runs

---

## Workflow State Machine

### Website Lifecycle

```
Created → Journey Discovery Pending → Journey Discovered → 
Monitoring Enabled → [Active Monitoring] ↔ Paused → Archived
```

### Monitoring Run Lifecycle

```
Queued → Running → [Lighthouse Tests] → Analysis Pending → 
Analyzed → Report Generated → Complete
                ↓
              Failed (with partial data)
```

---

## Data Retention & Storage Strategy

**Database (PostgreSQL):**
- Website metadata, journey definitions, run metadata
- Performance metrics (structured)
- Report metadata/references

**Object Storage (S3/MinIO):**
- Full Lighthouse JSON reports
- Generated PDF/Markdown reports
- Screenshots, trace files
- Reusable automation scripts

**Retention Policies:**
- Metrics: Indefinite
- Raw Lighthouse JSON: 90 days
- PDF reports: 1 year
- Screenshots: 30 days

---

## Performance & Scalability Considerations

- Max 5 simultaneous Lighthouse runs
- Queue-based processing for high demand
- Priority queue for on-demand vs scheduled runs
- Browser automation in isolated containers
- Automatic cleanup after 10-minute timeout
- Rate limiting: 100 API requests/min/user
- Cache AI-generated descriptions
- Compress stored reports
- Archive old runs to cold storage after 6 months

---

## Error Handling & Recovery

- Journey discovery: Retry up to 3 times, exponential backoff, manual fallback
- Lighthouse test: Mark failed steps, continue, store partial results, manual retry
- AI analysis: Save raw metrics, queue for manual review, allow re-triggering analysis

---

## Integration Points

- **MCP Servers**: Playwright/Puppeteer for browser automation
- **LLM APIs**: Journey discovery, report generation
- **Object Storage**: S3-compatible for artifacts
- **Future**: CI/CD webhooks, Slack notifications, Grafana dashboards

---

## Security & Access Control

**Current:**
- No authentication (internal use)
- Read-only API endpoints for reports
- Validate all external URLs before testing

**Future:**
- JWT-based authentication
- Role-based access control
- API keys for programmatic access
- Audit logs for all monitoring runs

---

## Key Performance Indicators (KPIs)

**System Health:**
- Journey discovery success rate (>95%)
- Lighthouse test completion rate (>98%)
- Average run completion time (<5 min)
- Report generation success rate (>99%)

**Business Value:**
- Number of issues identified/month
- Average time saved vs manual testing (~30 min/run)
- % of recommendations implemented
- Performance improvement trends for monitored websites

---